[{"content":"La gazza ladra 贼鹊 La gazza ladra 樱之刻的故事从《樱之诗》中多次在话语中提到的鸟谷静流的视角开始展开。\n“奇美拉”在最后一名代理店长怀孕离职之后，终于迎来了真正主人的回归，十年间环游世界的鸟谷静流回到了阔别已久的弓张，见到的第一位客人，便是主人公草薙直哉。随着与主人公的碰面，这位活在真琴口中的“表姐”，“奇美拉”真正的店长，夏目蓝的挚友，为故事带来新的可能性，本章即是从静流的视角讲述其就读弓张学园高三时发生的一系列事件。\n鸟谷静流是真琴的表姐，鸟谷真希是其姑妈，在静流就读时期的弓张学园由中村一族掌权，彼时中村章一之妹中村丽华高三，夏目蓝在真希帮助下进入高一，在这一年里，一场关系《樱之诗》故事走向的事件正在靠近。\n静流高三时喜爱陶艺，弓张的土质特殊，某些土壤非常适合进行烧瓷。静流跟着彼时的美术部老师学习烧瓷，偶然间，发现了美术部中的一部烧瓷作品竟然在光线的变化下能从碧色变为绯红色，而作品主人却不见其踪。\n中村丽华与静希是青梅竹马，丽华性格乖张，血脉意识强烈，在家庭环境的影响下，从小拥有强烈的审美批判意识，对真品赝品拥有绝对的判别能力。\n新的艺术形式的诞生必然伴随新的批评 而故事的引子便从两人的约定开始\u0026ndash;\n”你的作品会成为新的艺术，而那一天，我的批评究竟会是诋毁还是赞美“\n真品赝品之论，便是本章的主要讨论对象，而丽华这种审美直接影响了其价值观，即对血脉的高低贵贱极度敏感，丽华并非不能判断事物的正确与否，而恰好，她能理解自己举动的错误性，然而，这对于拥有强烈荣辱信念的她是可以扭曲的，这也是这位角色走向悲剧的无法调解之矛盾。\n失去信念 丽华和静流的微妙关系，将随着夏目蓝的闯入而出现变动。\n丽华重视血脉家族，而对中村一族造成了严重损失的原侧室后代以及那座夏目宅的后人无比敌视，扭曲的执念让丽华不惜以性威胁相逼，这一系列事件被静流撞破，由于静流本性与丽华不合，两人关系几近破裂，静流与夏目蓝在交流中成为好友。\n而彼时，一场决定多人命运的“政变”正在靠近。\n中村真希发动学校政变，检举理事长中村章一的丑闻，逼迫股东大换血，将中村一族赶出弓张学园，并与中村章一离婚，恢复”鸟谷“之姓。以此为标志，丽华和静流的关系正式宣告破裂，而中村一家遭受严重打击，中村丽华被迫退学。而与之而来的便是鸟谷家十年间难以恢复的母女关系。\n真希离婚只带走了真琴，而夏目圭仍然留在中村家，在真琴的拜托下，静流介入，想通过丽华带走夏目圭。这时，夏目圭的真实身世浮出水面。夏目圭生母恩田氏为美术世家恩田一族，而圭自小便表现出强烈的美术天赋，更是阴差阳错下在恩田一族下的美术学园进行培养，这一事件引发了中村一族的调查，发现了圭的真实身世，由于其血脉的价值，中村一族便不可能轻易让步放任圭离开。\n至此，故事主线明朗，静流为救圭重新接近丽华，在真希的提醒下有了“贼雀”的计划。也是在那个暑假，静流为制作出能骗过丽华的烧瓷而踏上异国之旅。\n时而迷恋上赝品，时而为赝品所欺骗的人，才会拥有一双能够鉴别真伪的眼睛。 在“碧绯”的启发下，在国外进修长达一年半的“暑假”后，静流完成了“真正的赝品”《雪景鹊图花瓶》。\n真实的赝品 而彼时丽华进入大学并与本间家订立婚约，失去了自己的信念，而失去信念无人可依的她收到了来自静流的来信，丽华向静流吐露心声，中村一族早已日薄西山，自己在本间家只能忍气吞声，早已失去了往日的乖张。我无法知道丽华真的是否为“赝品”所骗，但至少，她的话是真正对这件作品的赞美，这件作品正是往日与静流约定的“新的艺术”.\n因为它是完美的，所有我才能断定是真品 在流露的心声和不绝于耳的赞美面前，静流心里产生欺骗昔日好友的负罪感，在这种负罪感下，见到了夏目宅门口的那副《Olympia》，心里的不安被彻底激发，《Olympia》也正是真迹的“赝作”，在夏目蓝的帮助下，静流见到了这副“赝作”的主人，草薙健一郎。\n回顾故事，整个故事的暗线围绕草薙健一郎展开，从最开始在美术教室发现“碧绯”，到和夏目蓝发现黑白的“彩色”素描，到产生利用“赝作”欺骗丽华，最后见到《Olympia》，这一切都像是在追随那个男人的脚步，终于，在这章故事的矛盾达到最高点时，这个脚步指引静流来到健一郎面前，就像这个男人会告诉静流所有答案一般。而事实也是如此，这位天才艺术家一针见血的指出了“真迹与赝作”，“正义与负罪感”的破局之处，赝作存在本身的意义就是代替真作，因此无需为赝作而感到负罪，静流复杂的心情不是出于负罪感，而是对当年与丽华的约定取得胜利的喜悦\u0026ndash;鸟谷静流的作品，打动了拥有真伪鉴别之眼的中村丽华，因此，她感到喜悦。并且告诉她无需担心圭，圭和任何中村家侧室的孩子一样，自出生就以自己的方式在反抗，像水菜一样，像蓝一样，总有一天，他会以自己的方式将自己拯救出来。\n罪孽深重的男人 故事的最后，静流像丽华道出了《雪景鹊图花瓶》的真实来历。而即便知道真相后的丽华仍然相信那就是独一无二的真品，此刻她的真伪鉴别之眼是否也在发挥作用呢？我思考不出，即便整个世界都否定那是真迹，她也会相信那不是赝作，是出于对“新的艺术”的审美，还是在众叛亲离之际收到某人的关心而真正感到喜悦之情的守护呢？我不知道，无论如何，中村丽华选择守护那份感情，即便那份感情已经被静流宣告是谎言，即便那个作品将被世界认定为赝作。\n如果这个花瓶也是谎言，那一晚的那句关心也随之会变成谎言. 不得不说，扶她自的构思是真的惊人，在这样的非宏大叙事里写出了无与伦比的宿命感。让人感叹，人与人之间的“因果交流”竟是如此奇迹般的存在。而后来，大家都知道，在《樱之诗》的《picapica》里，丽华如约带走了那个花瓶，像十年前一样，那个人的《雪景鹊图花瓶》，对她来说就是独一无二的真品。\nLa gazza ladra 作为《樱之刻》的第一篇，本故事以回忆录的形式从鸟谷静流的视角补全了人物的关系与身世（特别是夏目圭），揭开了”弓张学园政变“时间点附近的故事，回忆录的口吻增添了宿命感，同时引出了本作另一个关键姓氏\u0026ndash;“本间”，是一个很好的引子。\n","date":"2024-08-29T18:22:35+08:00","image":"http://localhost:1313/img/4featureimg/cover.png","permalink":"http://localhost:1313/p/%E5%96%9C%E9%B9%8A%E7%9B%97%E8%B5%B0%E4%BA%86%E9%93%B6%E6%B1%A4%E5%8C%99/","title":"喜鹊盗走了银汤匙"},{"content":"","date":"2024-08-29T14:34:04+08:00","permalink":"http://localhost:1313/p/cvyolo%E7%AE%80%E5%8D%95%E5%AE%9E%E4%BE%8B/","title":"【CV】YOLO简单实例"},{"content":"LDA(线性判别分析) [TOC]\n1.LDA是什么 LDA是一种解决二分类问题的线性方法。它描述，对于给定样例集，将样例点投影到一条直线上，这条直线能使异样的样例相距远，同类的样例分布靠近，对于新的样例，根据在这条直线上的投影判断属于哪一类别。\n因此我们的所有任务围绕确定直线展开。\n2.问题背景 首先描述问题背景，这里直接引用西瓜书原话：\n这里描述的是一个二分类问题。\n那么如何理解投影？\n3.投影 若已知向量$\\vec{x}$和向量$\\vec{w}$ ,求$\\vec{x}$在向量$\\vec{w}$上的投影，可以用内积表示： $$ \\vec{x} \\cdot \\vec{w} = |\\vec{x}||\\vec{w}|\\cos {\\theta} $$ 当w为单位向量，该投影为： $$ |\\vec{x}|\\cos {\\theta} $$ 因此在下图上，$y$表示target（标签），假设x与y有线性关系由参数集合$w$确定（$y = wx + b，w^T = {w,b}*$）\n则任意x在直线上的投影可以认为是x根据线性关系找到的y值，那么这个投影过程表示为： $$ w^TX $$ 其中$x_i$在向量$X$方向上($X={X_1;X_2..;X_i}$)\n4.离散度 前面提到需要使得“异样的样例相距远，同类的样例分布靠近”，因此我们需要一个衡量标准，异样的距离使用类间散度衡量，同样使用类内散度衡量\n$ \\ {\\mu}_i$用来表示各类的均值，这里只有$\\ {\\mu_0},{\\mu_1}$,分别表示正类和负类的均值。异类之间的距离使用均值在直线的投影的距离表示： $$ ||w^T\\mu_0-w^T\\mu_1||_2^2 = w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw $$ 这里下标2表示2类向量的模，即欧几里得距离\n同类之间使用协方差比较距离： $$ w^T(\\Sigma_0 + \\Sigma_1) w $$ $\\ {\\Sigma}$为协方差矩阵\n为了简化表示，我们引入两个新概念，类间散度矩阵和类内散度矩阵。\n类间散度矩阵用$\\ {S_b}$表示： $$ S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T $$ 类内散度矩阵用$S_w$表示： $$ S_w= \\Sigma_0+\\Sigma_1 $$\n5.目标函数 为了同时考虑”使同类样例的投影点尽可能接近，可以让同类样例投影点的协方差尽可\n能小“，设置目标函数： $$ J=\\frac{w^TS_bw}{w^TS_ww} $$ 求这个目标函数的最大值可以转换为求$\\ S_b$和$\\ S_w$的**”广义瑞利商“**，这里使用拉格朗日乘子法求解，具体过程不在讨论范围。\n$$ min\\quad ω^TS_bw\\ s.t.\\quad w^TS_ww = 1 . $$ 最终求得$\\ w = S^{-1}(\\mu_0-\\mu_1)$\n6.推广到多分类 在多分类问题中，LDA一般作为降维方法进行属性约简。设target数量为N,$\\mu$为所有数据的均值，$\\ {\\mu_i}$表示示属性i的均值,$m_i$表示第i属性的数据量。\n首先定义”全局散度矩阵“： $$ S_t=S_b+S_w=\\sum_{i=1}^m({x_i}-\\mu)({x_i}-\\mu)^T $$ $\\ {S_w}$ 可以表示为： $$ {S_w}i = \\sum{x\\in X_i} \\Sigma_i = \\sum_{x\\in X_i} \\ (x-\\mu_i)(x-\\mu_i)^T\\ S_w = \\sum_{i=1}^N S_{wi} $$\n$\\ {S_b}$可以表示为： $$ S_b=\\sum_{i=1}^Nm_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T $$ 推导参考：\n多分类 LDA 可以有多种实现方法，使用 $S_w$, $S_t$ 两者中的任何两个即可。常见的一种实现是采用优化目标\n$$ max_W{\\frac{tr(W^TS_bW)}{tr(W^TS_wW)}} $$ $tr(\\cdot)$表示矩阵的迹（trace）即矩阵对角线上元素的和，我们在LDA中要做的是找到一个投影矩阵$W$，使得这个比值最大化。\n该式可以转换为一个最大广义特征值的问题的求解： $$ S_bW=\\lambda S_wW $$ $W$的闭式解则是 $S_w ^{-1}S_b$_的N-1个最大广义特征值所对应的特征向量组成的矩阵，即我们要求的投影矩阵\n","date":"2024-08-28T21:47:19+08:00","permalink":"http://localhost:1313/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80lda/","title":"【深度学习基础】LDA"},{"content":"决策树 回顾线性模型 在线性模型中，对于一个拥有i个属性的数据集我们通过构造$y = WX(W = (w,b)^*,X = (x_1;x_2;\u0026hellip;;x_i)$根据均方方差最小化或者对数几率最大化来进行线性回归或者对数几率回归来确定$W$，得到模型从而模拟得到输出y与数据集$X$的线性关系，其中对数几率回归由于输出的是概率因此被用作分类任务的一部分。而线性判别分析通过类间散度矩阵$S_b$和类内散度矩阵$S_w$拟合一个目标函数使得$S_b$和$S_w$的广义瑞利商最大，最后求得$W$的闭式解，通过将输出的值离散化为类别从而达到分类的目的。具体来说：\n在线性模型中，对于一个拥有 $i$个属性的数据集，我们通过构造 $$y = WX$$（其中 $$W = (w_1, w_2, \\ldots, w_i, b)^T$$，$$X = (x_1, x_2, \\ldots, x_i, 1)^T$$）(x1,x2,…,xi 是输入数据的特征)，根据最小化均方误差来进行线性回归，确定 $$W$$ 的取值，从而拟合模型，得到输出 $$y$$ 与数据集 $$X$$ 之间的线性关系。\n对数几率回归通过构造 $$P(y=1|X) = \\sigma(WX)$$（其中 $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$ 是Sigmoid函数），输出的是概率，因此被用作分类任务的一部分。模型通过最大化对数似然函数来确定 $$W$$ 的取值，通常使用迭代优化算法而非闭式解。\n线性判别分析通过类间散度矩阵 $$S_b$$ 和类内散度矩阵 $$S_w$$ 拟合一个目标函数，使得 $$S_b$$ 和 $$S_w$$ 的广义瑞利商最大，最后求得 $$W$$ 的闭式解($S_w ^{-1}S_b$_的N-1个最大广义特征值所对应的特征向量组成的矩阵),。通过将输出的值离散化为类别从而达到分类的目的。\n决策树的引入 在分类问题中，决策树比线性模型生动得多。决策树是一种划分策略，它通过一系列二分判断划分数据空间，并生成树状结构，每个节点表示一次决策后的数据集，边表示决策后数据集的分裂，叶子节点为最终输出的类别标签。决策树的引入是自然的，符合人们认识事物的规律，当判断一个瓜是好瓜还是坏瓜，我们根据西瓜的颜色瓜蒂等属性进行一系列二分，最后形成一套仅通过输入属性以及属性值就能判断瓜好坏的“模型”。（属性与特征在这里同义）\n以下是一个简单的西瓜数据集示例，包含一些离散属性和属性值：\n色泽 根蒂 敲声 纹理 脐部 触感 好瓜 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是 青绿 稍蜷 浊响 清晰 稍凹 软粘 是 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否 青绿 硬挺 清脆 清晰 平坦 软粘 否 浅白 硬挺 清脆 模糊 平坦 硬滑 否 浅白 蜷缩 浊响 模糊 平坦 软粘 否 青绿 蜷缩 浊响 稍糊 凹陷 硬滑 否 浅白 蜷缩 浊响 清晰 稍凹 硬滑 否 乌黑 稍蜷 沉闷 稍糊 稍凹 软粘 否 这个表格展示了每个西瓜样本的属性及其对应的好瓜（是）或坏瓜（否）标签。\n决策树构建 决策树的构建可以这样描述：\n选取最优特征； 分割数据集； 在以下三种情况退出分割： 当所有数据属于同一类别； 数据集的属性为空或者属性值全部相同； 数据集无法继续分割（样本数据量小于我们设定的某个阈值） 选取策略 上面我们提到了决策树的具体算法，大部分都很好实现，只有\u0026quot;选取最优特征\u0026quot;存在疑惑\u0026ndash;什么是最优特征？\n为了判断什么是最优特征，我们需要引入一些量化指标进行评估。\n信息增益 色泽 根蒂 敲声 纹理 脐部 触感 好瓜 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是 青绿 稍蜷 浊响 清晰 稍凹 软粘 是 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否 青绿 硬挺 清脆 清晰 平坦 软粘 否 浅白 硬挺 清脆 模糊 平坦 硬滑 否 浅白 蜷缩 浊响 模糊 平坦 软粘 否 青绿 蜷缩 浊响 稍糊 凹陷 硬滑 否 浅白 蜷缩 浊响 清晰 稍凹 硬滑 否 乌黑 稍蜷 沉闷 稍糊 稍凹 软粘 否 信息熵(Entropy)是用来度量样本集合纯度的指标，假设定当前样本集合$D$中第$k$类样本所占的比例为 $P_k$ (k = 1, 2,. . . , $|Y|$)，在这里我们可以将其表示为： $$ Entr(D)=-\\sum_{k=1}^{|Y|}p_klog_2p_k\\ . $$ 其中Ent(D)的值越小，则D越纯，这也很符合直觉，因为熵越大表明混乱程度越高，携带信息越多，而分类目的就是减少混乱程度。\n因此我们可以利用熵来表示当前数据集合的纯度，假设离散属性$a$有$V$个可能的取值{${a^1,a^2,a^3,a^4\u0026hellip;}$}，当根据属性$a$进行划分会产生$V$个分支节点，第$v$个节点包含$D$中所有属性$a$为$a^v$的数据集，记作$D^v$，我们为所有划分后的子集分配权重，并与进行划分前的信息熵作差就得到了我们第一个参考指标，信息增益（Gain）: $$ Gain(D,a) = Ent(D) -\\sum_{k=1}^{|Y|}\\frac{|D^v|}{|D|}Entr(D^v) \\ . $$ 这里给出规律：\nGain越大表明使用$a$进行属性划分获得的纯度增益越大 从而我们有了第一个评估指标。\n增益率 编号 色泽 根蒂 敲声 纹理 脐部 触感 好瓜 1 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是 2 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是 3 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是 4 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是 5 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是 6 青绿 稍蜷 浊响 清晰 稍凹 软粘 是 7 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是 8 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是 9 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否 10 青绿 硬挺 清脆 清晰 平坦 软粘 否 11 浅白 硬挺 清脆 模糊 平坦 硬滑 否 12 浅白 蜷缩 浊响 模糊 平坦 软粘 否 13 青绿 蜷缩 浊响 稍糊 凹陷 硬滑 否 14 浅白 蜷缩 浊响 清晰 稍凹 硬滑 否 15 乌黑 稍蜷 沉闷 稍糊 稍凹 软粘 否 思考这样一个问题，假设编号也是一个属性，我们将编号作为划分依据，这样的结果如何呢？\n从结果上不难想到，这样划分得到的数据集只包含一个数据样例，因此这样划分的纯度最高，但根据经验判断，编号本身对西瓜好坏程度是没有关联的，因此选取编号作为划分依据是错误的做法，为了规避信息增益对可取数目较多的属性有所偏好，于是引入增益率（Gain Ratio）： $$ Gain_ratio(D,a) = \\frac{Gain(D,a)}{IV(a)},\\ IV(a) = -\\sum_{v=1}^{V}\\frac{|D^v|}{|D|}log_2\\frac{|D^v|}{|D|} $$ $IV(a)$称为属性$a$的固有值(instrinic value)，可以看到当属性$a$可能的取值越多，$IV(a)$越大，有效规避信息增益对可取数目较多的属性有所偏好。\n增益率在C4.5决策树作为择优标准，增益率越大，划分效果越好。\n基尼指数 假设定当前样本集合$D$中第$k$类样本所占的比例为 $p_k$ (k = 1, 2,. . . , $|Y|$)，定义基尼值: $$ Gini(D)=1-\\sum_{k=1}^{|Y|}p_k^2\\ . $$ 前面我们通过定义数据的纯度引入信息熵，通过信息熵构造的信息增益和增益率来作为判断纯度增加的依据，在这里，我们不使用信息熵，反而通过描述一个数据集之间随机抽取两个样本，通过计算两个样本的标签类别不一致的概率来作为判断纯度的依据，因此基尼值的引入也很自然\n所以，假设离散属性$a$有$V$个可能的取值{${a^1,a^2,a^3,a^4\u0026hellip;}$}，当根据属性$a$进行划分会产生$V$个分支节点，第$v$个节点包含$D$中所有属性$a$为$a^v$的数据集，记作$D^v$，我们为所有划分后的子集分配权重，就得到了基尼指数： $$ Gini_index(D,a) = \\sum_{v=1}^V\\frac{|D^v|}{|D|}Gini(D^v)\\ . $$ $ Gini(D)$ 越小，则数据集$D$的纯度越高.\n决策树的构建 将上述数据集合抽象为特征矩阵$X$和标签向量$y$，特征矩阵将使用数字将特征值编号，列代表特征，标签向量为特征矩阵每一行对应一个分类标签，类似如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 X = np.array([ [0, 0, 0, 0, 0, 0], # 青绿, 蜷缩, 浊响, 清晰, 凹陷, 硬滑 [1, 0, 0, 0, 0, 0], # 乌黑, 蜷缩, 浊响, 清晰, 凹陷, 硬滑 [2, 0, 0, 0, 0, 0], # 浅白, 蜷缩, 浊响, 清晰, 凹陷, 硬滑 [0, 1, 1, 0, 1, 1], # 青绿, 稍蜷, 沉闷, 清晰, 稍凹, 软粘 [1, 1, 1, 1, 1, 1], # 乌黑, 稍蜷, 沉闷, 稍糊, 稍凹, 软粘 [2, 1, 1, 1, 1, 0], # 浅白, 稍蜷, 沉闷, 稍糊, 稍凹, 硬滑 [0, 2, 2, 2, 2, 1], # 青绿, 硬挺, 清脆, 模糊, 平坦, 软粘 [1, 2, 2, 1, 2, 0], # 乌黑, 硬挺, 清脆, 稍糊, 平坦, 硬滑 [2, 2, 2, 2, 2, 1] # 浅白, 硬挺, 清脆, 模糊, 平坦, 软粘 ]) # 标签向量 y y = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0]) # 1: 好瓜, 0: 坏瓜 接下来直接给出代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class Node: def __init__(self, feature_index = None, feature_val=None, left=None, right=None,value=None): self.feature_index = feature_index self.feature_val = feature_val self.left = left self.right = right self.value = value class DecisionTree(object): def __init__(self, criterion = \u0026#39;gini\u0026#39;, max_deepth = None, min_sample_split = None, root = None): self.root = root self.criterion = criterion self.max_deepth = max_deepth self.min_sample_split = 2 def _caculate_gini(self, X, y, feature_index, feature_val): \u0026#39;\u0026#39;\u0026#39; 计算基尼指数， 基尼值：计算所有标签的1-p_k^2之和, 基尼指数：在每个属性（feature_index）下根据属性值(feature_val)分配权重计算基尼值之和 \u0026#39;\u0026#39;\u0026#39; left = X[:, feature_index] \u0026lt;= feature_val right = X[:, feature_index] \u0026gt; feature_val y_left, y_right = y[left], y[right] # 计算基尼值 def gini(y_subset): classes, counts = np.unique(y_subset, return_counts = True) p_k = counts / len(y_subset) gini_val = 1 - sum(p_k** 2) return gini_val left_gini = gini(y_left) right_gini = gini(y_right) #计算加权值 total_gini = (len(y_left)/len(y))*left_gini+(len(y_right)/len(y))*right_gini return total_gini def _split_node(self, X, y, criterion = \u0026#39;gini\u0026#39;): \u0026#39;\u0026#39;\u0026#39; 将X进行划分，根据gini指数等，返回最佳划分方案， 为一个包含gini指数，最佳划分属性编号和最佳划分属性值的的三元组 \u0026#39;\u0026#39;\u0026#39; best_criterion = float(\u0026#39;inf\u0026#39;) if criterion == \u0026#39;gini\u0026#39; else -float(\u0026#39;inf\u0026#39;) best_feature_index = None best_feature_val = None _,n_features = X.shape for feature_index in range(n_features): feature_vals = np.unique(X[:,feature_index]) # 如果判断其是gini指数 for feature_val in feature_vals: if criterion == \u0026#39;gini\u0026#39;: gini = self._caculate_gini(X, y, feature_index, feature_val) # 更新最优划分 if gini \u0026lt; best_criterion: best_criterion = gini best_feature_index = feature_index best_feature_val = feature_val # 如果判断器是gain增益率 elif criterion == \u0026#39;gain\u0026#39;: gain = self._caculate_gain(X, y, feature_index, feature_val) # 更新最优划分 if gain \u0026gt; best_criterion: best_criterion = gain best_feature_index = feature_index best_feature_val = feature_val return best_criterion, best_feature_index,best_feature_val def _most_common_label(self, y): return np.bincount(y).argmax() def _build_tree(self, X, y, depth = 0, pred = 0): \u0026#39;\u0026#39;\u0026#39; 构建树,传入特征向量X和标签向量y \u0026#39;\u0026#39;\u0026#39; # 首先设置停止划分的条件，叶子节点保存输出类别，类别为当前标签集合中最常见的标签 n_samples, n_features = X.shape# 行数正好是数据总数，列数为属性的总数目 n_labels = len(np.unique(y)) if n_labels == 1 or depth \u0026gt;= self.max_deepth or n_samples \u0026lt; self.min_sample_split: leaf_val = self._most_common_label(y) return Node(value=leaf_val) # 开始划分 _, feature_index, feature_val = self._split_node(X, y, \u0026#39;gini\u0026#39;) # 设置划分的开始编号 left_idx = X[:,feature_index] \u0026lt;= feature_val right_idx = X[:,feature_index] \u0026gt; feature_val left_pred = np.sum(y[left_idx] == 0) / len(y[left_idx]) right_pred = np.sum(y[right_idx] == 0) / len(y[right_idx]) # 预剪枝，递归处理左右子集 if pred \u0026gt;= left_pred and pred \u0026gt;= right_pred: leaf_val = self._most_common_label(y) return Node(value=leaf_val) else: left = self._build_tree(X[left_idx,:],y[left_idx], depth+1, left_pred) right = self._build_tree(X[right_idx,:], y[right_idx],depth+1, right_pred) if left is None and right is None: leaf_val = self._most_common_label(y) return Node(value=leaf_val) # 返回树结构 return Node(feature_index=feature_index,feature_val=feature_val,left=left, right=right) def _traverse_tree(self, x, node): \u0026#39;\u0026#39;\u0026#39; 遍历树以匹配输入数据x的输出标签类别 \u0026#39;\u0026#39;\u0026#39; if node.value is not None: return node.value if x[node.feature_index] \u0026lt;= node.feature_val: return self._traverse_tree(x, node.left) return self._traverse_tree(x, node.right) def fit(self, X, y): self.root = self._build_tree(X,y) #将树保存再在oot中 def predict(self, X): return [self._traverse_tree(x, self.root) for x in X] 剪枝 预剪枝 预剪枝发生在划分子集时，如果划分子集之和，精度（这里指子集中正类的占比）没有提升，那么这次划分就是需要舍去的\n1 2 3 4 5 6 7 8 9 10 11 12 13 left_pred = np.sum(y[left_idx] == 0) / len(y[left_idx]) right_pred = np.sum(y[right_idx] == 0) / len(y[right_idx]) # 预剪枝，递归处理左右子集 if pred \u0026gt;= left_pred and pred \u0026gt;= right_pred: leaf_val = self._most_common_label(y) return Node(value=leaf_val) else: left = self._build_tree(X[left_idx,:],y[left_idx], depth+1, left_pred) right = self._build_tree(X[right_idx,:], y[right_idx],depth+1, right_pred) if left is None and right is None: leaf_val = self._most_common_label(y) return Node(value=leaf_val) 后剪枝 后剪枝先从训练集生成一棵完整决策树，计算决策树的精度，然后自底向上地将节点替换为叶子节点，如果精度上升，则保留叶子节点，如果下降，则撤回操作，继续对其他节点进行操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 后剪枝 def _prune_tree(self, node, X_val, y_val): if node is None: return None # 递归地剪枝左子树和右子树 node.left = self._prune_tree(node.left, X_val, y_val) node.right = self._prune_tree(node.right, X_val, y_val) # 如果当前节点的左右子树都为空，返回当前节点 if node.left is None and node.right is None: return node # 获取当前节点的值 new_leaf_val = node.value # 如果左右子树都存在，则尝试剪枝 if node.left is not None and node.right is not None: # 保存原始树 original_tree = self # 创建一个新的叶子节点，其值为当前节点的值 new_node = Node(value=new_leaf_val) self.root = new_node # 计算剪枝前后模型在验证集上的准确度 if self._evaluate_tree(X_val, y_val) \u0026gt; self._evaluate_tree(original_tree, X_val, y_val): # 如果剪枝后的树在验证集上的表现更好，返回新的叶子节点 return new_node else: # 否则，恢复原始树并返回原始节点 self.root = node return node 在写训练函数fit时作出适当改动：\n1 2 3 4 def fit(self, X, y, X_val = None, y_val = None, prune = False): self.root = self._build_tree(X,y) #将树保存再在root中 if prune and X_val is not None and y_val is not None: self._prune_tree(self.root, X_val, y_val) ","date":"2024-08-28T11:37:37+08:00","permalink":"http://localhost:1313/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B/","title":"【深度学习基础】决策树模型"},{"content":"神经网络（neural networks） 神经元 感知机模型（Perceptron） 感知机模型 感知机是由神经元复合的线性分类器。 $$ y =f(\\sum_{i=1}^n w_ix_i -\\theta) = f(w^Tx-\\theta) $$ 其中：\nx为样本的特征向量。 w为权重向量。 θ称为偏置（阈值） 如何这里的$f$为阶跃函数$\\epsilon(·)$，则感知机模型可以表示为激活函数： $$ (1)y =\\epsilon(w^Tx-\\theta) =\\left{ \\begin{matrix} \\ 1,w^Tx\u0026gt;=\\theta\\ \\ 0,w^Tx\u0026lt;\\theta \\end{matrix} \\right. $$ 由$n$维空间中的超平面方程： $$ w_1x_1+w_2x_2+w_3x_3+\u0026hellip;+w_nx_n+b=w^Tx+b=0 $$ 可知$(1)$式将n维空间划分，可以看作一个超平面，将n维空间划分为$w^Tx\u0026gt;=\\theta$和$w^Tx\u0026lt;\\theta$两个空间，落在前者的输出值为1，后者的输出值为0，于是实现了分类功能。\n感知机的学习策略以及参数的调整规则 所以线性模型中的思考，我们需要对权重向量$w^T$和偏置$\\theta$进行选择，这就是感知机的学习\n不妨考虑一种简单的情况，现在有特征矩阵$x_1$=[2, 3]，$x_2$=[1,5]，包含标签的向量$y={1,0}$\n初始化权重向量$w$：假设$w=[1,1]$，偏置$\\theta=2$；\n计算输出：\n根据公式$(1)$, 有$w^Tx_1-\\theta=1·2+1·3-2=3\u0026gt;0$, 因此被判定为1，符合真实输出； 对于$x_2$，计算得到$w^Tx_2-\\theta = 6-2=4\u0026gt;0$，不符合输出，因此需要调整权重向量$w$和偏置$\\theta$ 提出以下优化规则： $$ w\u0026rsquo; = w+η(y-y_{pred})x\\ \\theta\u0026rsquo;=\\theta-η(y-y_{pred}) $$ ​\t假设模型的学习率$η$为1，于是$w\u0026rsquo; = w + η(0-1)·[1,5] = [0,-4]$,$\\theta\u0026rsquo; = 2 + 1 =3$\n对于$x_1$，再次计算$w^Tx_1-\\theta= -15 \u0026lt; 0$，与真实输出不符； 对于$x_2$，易得$w^Tx_1-\\theta = -23 \u0026lt; 0$，输出为0符合标签 由于输出仍不满足，因此继续根据规则调整权重和偏置验证计算。\n多层前馈神经网络 在前面我们提到，感知机是一种线性分类器，利用感知机模型构成的单层神经元只有输出层神经原进行激活函数处理，只能解决线性可分的问题。\n其中“与”“或”“非”都是线性可分的，即存在一个线性超平面能将他们分开，在感知机的学习过程中调整权重矩阵和偏置最终使得学习过程收敛（converge），而对于非线性可分的问题，例如图上的\u0026quot;异或\u0026quot;问题，可以看到不存在有且仅有一个线性超平面将他们分开，这使得学习过程发生振荡（fluctuation），无法确定解。\n含隐藏层的多层功能神经元 为了解决非线性可分问题，我们引入多层功能神经元的概念，这里两层感知机模型就能解决上“异或”问题.\n更一般的多层功能神经元构成的神经网络如图所示：\n每层神经元与下一层神经元全互连，同层神经元不进行连接，也不跨层连接，上一层神经元的输出作为下一层神经元的输入，我们定义这样的神经网络为\u0026quot;多层前馈神经网络\u0026quot;（multi-layer feedback neural networks）.神经元之间用**“连接权”和阈值**进行连接。\n单隐层前馈网络是指只有一个隐藏层，双隐层前馈网络有两层隐藏层，需要注意的是，输入层神经元没有激活函数进行处理，仅仅作为输入。\n误差逆传播算法(BP神经网络) 由于构造了复杂的学习模型多层前馈神经网络，我们的单层感知机模型的学习方法$\\Delta w = η(y-y_{pred})x$\n$\\Delta\\theta = -η(y-y_{pred})$已经无法正常运行。因此我们需要更适用的算法。\n**误差逆传播算法(error BackPropagation，BP)**是一种适用于多层前馈神经网络的学习算法（BP算法不止适用于多层前馈神经网络）。\n我们来解释一下BP算法的具体过程。\n训练集定义：\n设定训练集 $(D = {(x_1, y_1), (x_2, y_2), \u0026hellip;, (x_m, y_m)})$，其中$ (x_i \\in \\mathbb{R}^d)，(y_i \\in \\mathbb{R}^l)$，即每个示例由 $d$个属性输入，输出 $l$ 维实值向量。 BP网络结构：\n图 5.7 展示了一个具有输入层、隐藏层和输出层的三层前馈神经网络： 输入层： 包含$ (d)$ 个输入神经元。 隐藏层： 包含 $(q) $个神经元。 输出层： 包含 $(l)$ 个神经元。 数学符号和公式：\n隐层神经元的输入： 第 $h$ 个隐层神经元接收到的输入为：$(a_h = \\sum_{i=1}^{d} v_{ih} x_i)$，其中 $w_{ih}$ 表示从输入层第 $i$ 个神经元到隐层第 $h$ 个神经元的连接权值。 输出层神经元的输入： 第 $j$ 个输出层神经元接收到的输入为：$(β_j = \\sum_{h=1}^{q} w_{hj} b_h)$，其中 ($b_h$) 是隐层第 ($h$) 个神经元的输出，($w_{hj}$) 是从隐层第 ($h$) 个神经元到输出层第 ($j$) 个神经元的连接权值。 激活函数：\n假设隐层和输出层神经元都使用 $Sigmoid $函数作为激活函数。 输出层的计算：\n对于训练例 $((x_k, y_k))$，假设神经网络的输出为 $$(y_{k1}^, y_{k2}^, \u0026hellip;, y_{kl}^)$$(表示第k个输入的第j个神经元输出)，即 $$(y_{kj}^ = f(β_j - θ_j))$$，其中 ($f$) 为激活函数，($θ_j$) 为第 ($j$) 个输出神经元的阈值。 网络中有 $(d+l+1)q+l$个参数需确定:输入层到隐层的$d\\times q$个权值，隐层到输出层的$q\\times l$个权 层神 的权值、 $q$ 个隐层神经元的偏置，$l$个输出层神经元的阈值。\n直到上面为止还是能沿用单层感知机的理解，接下来我们也同单层模型一样，考虑学习方法以确定权重矩阵和偏置。\n均方误差 为了找到一组好的参数值$(w,\\theta)$，我们需要一个评估指标描述什么是\u0026quot;好的\u0026quot;。此时我们想起线性回归模型的处理方法，这里我们适用均方误差来进行。\n对于训练例 $(x_k, y_k)$，定义网络在 $(x_k, y_k)$ 上的均方误差为： $$ E_k = \\frac{1}{2} \\sum_{j=1}^{l} (y_{kj}^* - y_{kj})^2 $$\n更新过程与梯度下降 $v$为任意参数，$v$的更新过程表示为更新估计式： $$ v \\leftarrow v+\\Delta v \\ . $$ 以隐层到输出层的连接权$w_{hj}$为例，使用梯度下降(gradient descent)策略，梯度下降在线性回归模型中也关键方法。给定学习率$η$，有： $$ \\Delta w_{hj} = -η\\frac{∂E_k}{∂w_{hj}} $$\n链式法则 我们可以注意到$E_k$是$w_{hj}$的复合函数，这里使用复合函数求导的链式法则，有： $$ (1)\\frac{∂E_k}{∂w_{hj}}=\\frac{∂E_k}{∂y^_{kj}}·\\frac{∂y^{kj}}{∂\\beta_j}·\\frac{∂\\beta_j}{∂w{hj}}\\ . $$ 由于$(β_j = \\sum_{h=1}^{q} w_{hj} b_h)$，所以： $$ (2)\\frac{∂\\beta_j}{∂w_{hj}} = b_h\\ . $$ 定义$g_j$: $$ (3)g_j =-\\frac{∂E_k}{∂y^_{kj}}·\\frac{∂y^{kj}}{∂\\beta_j}\\ =-(y^*{kj}-y_{kj})f\u0026rsquo;(\\beta_j-\\theta_j)\\ =y^_{kj}(1-y^{kj})(y{kj}-y^*{kj}) $$ 最终可以得到 $$ \\Delta w{hj} =ηg_jb_h\\ . $$ 类似可得其他参数： $$ \\Delta \\theta_j = - \\eta g_j， $$\n$$ \\Delta w_{ih} = \\eta e_h x_i， $$\n$$ \\Delta \\gamma_h = - \\eta e_h， \\quad $$\n$$ e_h = - \\frac{\\partial E_k}{\\partial b_h} \\cdot \\frac{\\partial b_h}{\\partial \\alpha_h} $$\n$$ = - \\sum_{j=1}^{l} \\frac{\\partial E_k}{\\partial \\beta_j} \\cdot \\frac{\\partial \\beta_j}{\\partial b_h} f\u0026rsquo;(\\alpha_h - \\gamma_h) $$\n$$ = \\sum_{j=1}^{l} w_{hj} g_j f\u0026rsquo;(\\alpha_h - \\gamma_h) $$\n$$ = b_h (1 - b_h) \\sum_{j=1}^{l} w_{hj} g_j。 \\quad (5.15) $$\nBP算法手动实现 对每个训练样例，BP算法执行以下操作：\n（1）先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；\n（2）然后计算输出层的误差（第4-5行）\n（3）再将误差逆向传播至隐层神经元（第6行）\n（4）最后根据各层神经元的误差来对连接权和阈值进行调整（第7行）。\n该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差达到一个很小的值。图给出了在2个属性、5个样本的西瓜数据上，随着训练样例的增加，网络参数和分类边界的变化情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 输入：训练集 $D = \\{(x_k, y_k)\\}_{k=1}^m$ 学习率 $\\eta$ 过程： 1. 在 $[-1, 0, 1]$ 范围内随机初始化网络中所有连接权和阈值 2. repeat 3. for all $(x_k, y_k) \\in D$ do 4. 根据式(5.3)计算当前样本的输出 $y_{kj}^*$ 5. 根据式(5.10)计算输出层神经元的梯度 $g_j$ 6. 根据式(5.11)计算隐层神经元的梯度 $e_h$ 7. 根据式(5.12)-(5.14)更新连接权 $w_{hj}, v_{ih}$ 和阈值 $\\theta_j, \\gamma_h$ 8. until 达到停止条件 输出：连接权与阈值确定的多层前馈神经网络 附上手写的源码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 import numpy as np class nn: def __init__(self, input_size, hidden_size, output_size, eta): self.weight_1 = np.random.randn(hidden_size, input_size) self.weight_2 = np.random.randn(output_size, hidden_size) self.thre_1 = np.random.randn(hidden_size) self.thre_2 = np.random.randn(output_size) self.eta = eta def _get_input(self, weight, X): return np.dot(X, weight.T) def _sigmoid(self, input, thre): input = np.array(input) - thre return 1 / (1 + np.exp(-input)) def _get_output(self, input, thre): return self._sigmoid(input, thre) def _get_MSE(self, output, pre_output): return np.mean((pre_output - output) ** 2) / 2 def _get_new_weight(self, grad, X): delta_weight = self.eta * np.outer(grad, X) return delta_weight def _get_new_thre(self, grad): return -self.eta * np.sum(grad, axis=0) def _get_output_grad(self, output, y): error_output = output - y grad_output = error_output * output * (1 - output) return grad_output def _get_hide_grad(self, weight, output, grad_output): error_hidden = np.dot(weight.T, grad_output) grad_hidden = error_hidden * output * (1 - output) return grad_hidden def fit(self, X, y, num_epochs=1000): for epoch in range(num_epochs): # 前向传播 input_1 = self._get_input(self.weight_1, X) output_1 = self._get_output(input_1, self.thre_1) input_2 = self._get_input(self.weight_2, output_1) output_2 = self._get_output(input_2, self.thre_2) # 计算损失 loss = self._get_MSE(y, output_2) # 反向传播 grad_output = self._get_output_grad(output_2, y) grad_hidden = self._get_hide_grad(self.weight_2, output_1, grad_output) # 更新权重和偏置 delta_weight_2 = self._get_new_weight(grad_output, output_1) self.weight_2 -= delta_weight_2 self.thre_2 -= self._get_new_thre(grad_output) delta_weight_1 = self._get_new_weight(grad_hidden, X) self.weight_1 -= delta_weight_1 self.thre_1 -= self._get_new_thre(grad_hidden) if epoch % 100 == 0: print(f\u0026#34;Epoch {epoch}, Loss: {loss}\u0026#34;) def predict(self, X): input_1 = self._get_input(self.weight_1, X) output_1 = self._get_output(input_1, self.thre_1) input_2 = self._get_input(self.weight_2, output_1) output_2 = self._get_output(input_2, self.thre_2) return output_2 input_size = 4 hidden_size = 5 output_size = 3 eta = 0.7 model = nn(input_size, hidden_size, output_size, eta) X = np.array([0.5, 1.0, 1.5, 2.0]) y = np.array([0.1, 0.2, 0.3]) model.fit(X, y) prediction = model.predict(X) print(f\u0026#34;Prediction: {prediction}\u0026#34;) \u0026#39;\u0026#39;\u0026#39; Epoch 0, Loss: 0.08452484439144649 Epoch 100, Loss: 0.00043097425030784627 Epoch 200, Loss: 6.173784504106113e-05 Epoch 300, Loss: 6.111382932194645e-06 Epoch 400, Loss: 5.274844786786648e-07 Epoch 500, Loss: 4.445371701622411e-08 Epoch 600, Loss: 3.7266722662723944e-09 Epoch 700, Loss: 3.1195917455011153e-10 Epoch 800, Loss: 2.6102862257159366e-11 Epoch 900, Loss: 2.1838569148871845e-12 Prediction: [0.1000008 0.19999945 0.3000004 ] \u0026#39;\u0026#39;\u0026#39; 累积误差逆传播算法（累积BP） 前面介绍的BP算法针对单个均方误差$E_k$，这意味着每次参数更新都只针对一个样例数据，导致更新频繁以及可能出现的不同样例导致的参数调整抵消等问题。所以为了使数据集整体达到同样的误差极小点，累积BP直接针对累积误差最小化:$E = \\frac{1}{m} \\sum_{k=1}^{m}E_k$，读取数据集一遍之后再进行更新，然而累积BP在累积误差下降到一定程度时，可能出现下降缓慢的情况，这时标准BP会得到更好的解。\n类似随机梯度下降和标准梯度下降的区别。\n过拟合 早停：将数据集分成训练集和验证集，训练集用来计算梯度，更新连接权和偏置，验证集用来估计误差，如果训练集误差降低但是验证机误差升高，则停止训练（类似决策树的后剪枝） 正则化：在误差目标函数增加一个描述网络复杂度的部分，例如连接权和偏置的平方和： $$ E=\\lambda \\frac{1}{m}\\sum_{k=1}^m E_k + (1-\\lambda)\\sum_i w_i^*\\ , $$\n其中$\\lambda \\in (0,1)$用于经验误差与网络复杂度这两项进行折中。常使用交叉验证法来估计。\n","date":"2024-08-28T11:37:10+08:00","permalink":"http://localhost:1313/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/","title":"【深度学习基础】神经网络"},{"content":"支持向量机（SVM） [TOC]\n在感知机模型中，我们通过找到一个超平面$w^Tx - \\theta = 0$来将样本空间划分来完成分类任务，在感知机模型中，主要通过设置$w^T和\\theta$的更新公式完成超平面的确定。现在我们考虑最合适的超平面的确认方式\u0026ndash;超平面离样本尽可能远离并且正好处于“正中间”。这就是SVM想要求解的情形。\n间隔 在样本空间中，定义任意点$x$到超平面$(w,b)$的距离可写为： $$ r=\\frac{|w^Tx + b|}{||w||} $$ 将训练样本分类好的超平面应当满足：\n（1）离样本足够远\n（2）正好处于最中间\n根据超平面性质，y为1被划分到正空间，y为-1被划分到负空间： $$ \\left{ \\begin{matrix} w^Tx+b\u0026gt;=0,y=+1\\ w^Tx+b\u0026lt;0,y=-1\\ \\end{matrix} \\right. $$ 接下来满足（2）： $$ \\left{ \\begin{matrix}\n\\frac{|w^Tx_i+b|}{||w||}\u0026gt;\\frac{|w^Tx_^{+}+b|}{||w||}\\ \\frac{|w^Tx_i+b|}{||w||}\u0026gt;\\frac{|w^Tx_^{-}+b|}{||w||}\\ \\end{matrix} \\right. $$ 由两式得 $$ \\left{ \\begin{matrix} w^Tx+b\u0026gt;=1,y=+1\\ w^Tx+b\u0026lt;=-1,y=-1\\ \\end{matrix} \\right. $$ 当且仅当$x_i$为离超平面最近的点时。等号成立，这些样本点都是一个特征向量，这样的向量被称为“支持向量”。两个不同类的支持向量到超平面的距离之和为： $$ \\gamma = \\frac{2}{||w||} $$ 被称为间隔（margin），满足（1）意味着间隔最大\n到这里我们明确了，想要找到满足条件的超平面，要找到满足$\\left{ \\begin{matrix} w^Tx+b\u0026gt;=1,y=+1\\ w^Tx+b\u0026lt;=-1,y=-1\\ \\end{matrix} \\right.$的超平面的参数$w$和$b$使得$\\gamma$最大： $$ max_{w,b}\\frac{2}{||w||} \\ \\qquad s.t.\\quad y_i(w^Tx_i+b)\u0026gt;=1,\\quad i=1,2\u0026hellip;,m. $$ 要求解$\\frac{2}{||w||}$最大的情形，等价于求$||w||^2$最小的情形，因此上述问题等价于式（2）： $$ min_{w,b}\\frac{||w||^2}{2} \\ \\qquad s.t.\\quad y_i(w^Tx_i+b)\u0026gt;=1,\\quad i=1,2\u0026hellip;,m. $$\nps:这里转化为$||w||^2$而不是$||w||$的原因在于，求$||w||^2$是一个凸优化问题，更容易使用优化算法求解。\n$s.t.\\quad y_i(w^Tx_i+b)\u0026gt;=1,\\quad i=1,2\u0026hellip;,m.$是由$\\left{ \\begin{matrix} w^Tx+b\u0026gt;=1,y=+1\\ w^Tx+b\u0026lt;=-1,y=-1\\ \\end{matrix} \\right.$得到的\n对偶问题 对于式（2），我们这里采用拉格朗日乘子法求得“对偶问题”，对于约束添加拉格朗日乘子$\\alpha_i\u0026gt;=0$,有式（3）： $$ L(w,b,\\alpha) = \\frac{1}{2}||w||^2+\\sum_{i=1}^m \\alpha_i(1-y_i(w^Tx_i+b)) $$ 分别对$w$,$b$求偏导： $$ w=\\sum_{i=1}^m\\alpha_iy_ix_i\\ 0=\\sum_{i=1}^m\\alpha_iy_i\\ . $$ 带入原式得式（2）的对偶问题式（4）： $$ max_\\alpha\\sum_{i=1}^m-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j\\ \\ \\qquad s.t.\\quad \\sum_{i=1}^m\\alpha_iy_i=0,\\ \\alpha_i\u0026gt;=0,\\quad i=1,2\u0026hellip;,m. $$\n为什么求其对偶问题：\n（1） 式 （2） 中的未知数是$ w $和$ b$，式 (4) 中的未知数是 $α$，$w$ 的维度 $d$ 对应样本特征个数，$α$ 的 维度 $m$ 对应训练样本个数，通常 $m ≪ d$，所以求解式(4) 更高效，反之求解式 (2) 更高效\n（2）式(4)中有样本内积$x_i^Tx_j$这一项，后续可以很自然地引入核函数，进而使得支持向量机也能对在原始特征空间线性不可分的数据进行分类(pumpkin book)）\n解出$\\alpha$后，得到$w$与$b$，于是可以得到模型(*)： $$ f(x) = w^Tx+b\\ =\\sum_{i=1}^m\\alpha_iy_ix_i^Tx+b $$\n核函数 与单层感知层模型一样，上面的讨论我们假设了样本空间是线性可分的，因此可以由一个线性超平面来划分样本空间： $$ f(x) =w^Tx+b $$ 所以当面对“异或”这样的非线性可分任务时就无法运作，一个好的办法是将原本样本空间的特征映射到一个更高维的空间里去：\n根据投影定理，任何有限维空间都可以嵌入到更高维的空间中。这个更高维的空间包含了原始空间的所有特征，同时还可以容纳更多的特征，从而使得数据分析和处理变得更加灵活和准确。\n令$\\phi(x)$表示将$x$映射后的特征向量，那么划分样本空间的超平面表示为： $$ f(x)=w^T\\phi(x)+b $$ 类似的，求解$w^T$满足凸优化问题： $$ min_{w,b}\\frac{1}{2}||w||^2\\ s.t.\\quad y_i(w^T\\phi(x_i)+b)\u0026gt;=1,\\quad i=1,2,3,\u0026hellip;,m. $$ 类似地，也有对偶问题(): $$ max_\\alpha\\sum_{i=1^m}\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_j\\phi(x_i)^T\\phi(x_i)\\ s.t. \\quad \\sum_{i=1}^m\\alpha_iy_i = 0,\\quad \\alpha_i\u0026gt;=0,\\quad i= 1,2,..,m. $$ 在()中，显式地处理$\\phi(x_i)^T\\phi(x_i)$显得十分麻烦，因为升维之后的空间可能维度很高，甚至无限维(维数灾难)，所以我们考虑有函数$k(x_i,x_j)$，在升维之后在样本空间的内积能改写为在原始样本空间的$k(x_i,x_j)$： $$ max_\\alpha\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jk(x_i,x_i)\\ s.t. \\quad \\sum_{i=1}^m\\alpha_iy_i = 0,\\quad \\alpha_i\u0026gt;=0,\\quad i= 1,2,..,m. $$ 求解后可以得到： $$ f(x) = w^T\\phi(x)+b \\= \\sum_{i=1}^m \\alpha_iy_i\\phi(x_i)^T\\phi(x)+b \\ =\\sum_{i=1}^m\\alpha_i y_i \\kappa(x,x_i)+b\\ . $$ 这里的映射$k(·,·)$就是核函数（kwenel function），模型的最优解可以通过训练样本的核函数展开，这一展开式也叫做**\u0026ldquo;支持向量展式\u0026rdquo;（support vector expansion）**\n核函数$k$隐式地包含在空间里，我们无法直接知道合适的核函数的存在性以及具体形式。满足以下性质的函数被称为核函数。\n核函数的定义 核函数 ($\\kappa(x, y)$) 是一个定义在输入空间 ($\\mathcal{X}$) 上的函数，它满足以下条件：\n对称性：($\\kappa(x, y)$ = $\\kappa(y, x)$) 对于任意数据集 ($D = {x_1, x_2, \\ldots, x_m\\$)，对应的核矩阵 ( $K$ ) 是半正定的。 $$ K = \\begin{bmatrix} \\kappa(x_1, x_1) \u0026amp; \\kappa(x_1, x_2) \u0026amp; \\cdots \u0026amp; \\kappa(x_1, x_m) \\ \\kappa(x_2, x_1) \u0026amp; \\kappa(x_2, x_2) \u0026amp; \\cdots \u0026amp; \\kappa(x_2, x_m) \\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\ \\kappa(x_m, x_1) \u0026amp; \\kappa(x_m, x_2) \u0026amp; \\cdots \u0026amp; \\kappa(x_m, x_m) \\ \\end{bmatrix} $$\n核函数隐含地定义了一种将输入数据从原始空间映射到一个高维特征空间的方法。在这个高维特征空间中，原本在低维空间中非线性可分的数据可能变得线性可分。这种特征空间的映射是通过核技巧实现的，而不需要显式地计算映射后的特征。\n只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用.事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\\phi$. 换言之，任何一个核函数都隐式地定义了一个称为\u0026quot;再生核希尔伯特空间\u0026quot; (Reproducing Kernel Hilbert Space ，简称 RKHS) 的特征空间.\n常见核函数 以下是一些常见的核函数及其参数的表格：\n名称 表达式 参数 线性核 $\\kappa(x_i, x_j) = x_i^T x_j$) 无 多项式核 $\\kappa(x_i, x_j) = (x_i^T x_j + c)^d$) ($d \\geq 1$) 为多项式的次数，($c$) 为常数 高斯核 $\\kappa(x_i, x_j) = \\exp \\left( -\\frac{|x_i - x_j|^2}{2\\sigma^2} \\right)$) ($\\sigma \u0026gt; 0$) 为高斯核的带宽（$width$） 拉普拉斯核 $\\kappa(x_i, x_j) = \\exp \\left( -\\frac{|x_i - x_j|}{\\sigma} \\right)$) ($\\sigma \u0026gt; 0$) Sigmoid 核 $\\kappa(x_i, x_j) = \\tanh (\\beta x_i^T x_j + \\theta)$) ($\\beta \u0026gt; 0$, $\\theta \u0026lt; 0$) 以下是一些常见核函数组合方式及其公式：\n$$ \\text{若 } \\kappa_1 \\text{ 和 } \\kappa_2 \\text{ 为核函数，则对于任意正数 } \\gamma_1, \\gamma_2, \\text{ 其线性组合} $$\n$$ \\gamma_1 \\kappa_1 + \\gamma_2 \\kappa_2 \\tag{6.25} $$\n也是核函数；\n$$ \\text{若 } \\kappa_1 \\text{ 和 } \\kappa_2 \\text{ 为核函数，则核函数的直积} $$\n$$ (\\kappa_1 \\otimes \\kappa_2)(x, z) = \\kappa_1(x, z) \\kappa_2(x, z) \\tag{6.26} $$\n也是核函数；\n$$ \\text{若 } \\kappa_1 \\text{ 为核函数，则对于任意函数 } g(x), $$\n$$ \\kappa(x, z) = g(x) \\kappa_1(x, z) g(z) \\tag{6.27} $$\n也是核函数。\n这些性质表明，通过不同的组合方式，可以构造新的核函数以满足特定应用需求。常见的组合方法包括核函数的加.\n软间隔与正则化 在实际问题中，尽管样本空间可能满足线性可分，然而总有一些样本可能出现不满足约束的分布： 所以我们的模型应该使得SVM允许一些样本犯错，这样的约束条件称为软间隔，使得一些样本可以不满足$y_i(w^Tx_i+b)\u0026gt;=1$.\n正则化 为了描述好这个问题，我们可以将问题$min_{w,b}\\frac{1}{2}||w||^2$进行正则化表示： $$ \\min_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^ml_{0/1} (y_i(w^Tx_i+b)-1) $$ 其中$l_{0/1}$称为“0/1损失函数”： $$ l_{0/1}(z)= \\left{ \\begin{matrix} 1,\\quad z\u0026lt;0.\\ 0,\\quad otherwise.\\ \\end{matrix} \\right . $$ 可以看到当$C$接近$+\\infin$，$y_i(w^Tx_i+b)-1$必须严格满足间隔条件。\n由于\u0026quot;0/1损失函数\u0026quot;数学性质不好，所以有一些替代算是函数：\n以下是列出三种损失函数的表格，包含公式：\n损失函数 公式 Hinge 损失 $$\\ell_{\\text{hinge}}(z) = \\max(0, 1 - z)$$ 指数损失 $$\\ell_{\\text{exp}}(z) = \\exp(-z)$$ 对率损失 $$\\ell_{\\text{log}}(z) = \\log(1 + \\exp(-z))$$ 为了方便研究，引入松弛变量(slack variables): $$ \\epsilon_i = \\ell_{hinge}(y_i(w^Tx_i+b)) $$ 此时原优化问题的约束条件发生变化,以$Hinge$损失为例： $$ \\max(0,1-y_i(w^Tx_i+b)) = \\epsilon_i\\ \\ \\epsilon= \\left{ \\begin{matrix} 0,\\quad 1-y_i(w^Tx_i+b)\u0026lt;=0\\ 1-y_i(w^Tx_i+b),\\quad 1-y_i(w^Tx_i+b)\u0026gt;0 \\end{matrix} \\right .\\ \\ y_i(w^Tx_i+b)\u0026gt;=1-\\xi_i $$ 我们就得到了软间隔支持向量机： $$ \\min_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m\\xi_i\\ s.t.\\quad y_i(w^Tx_i+b)\u0026gt;=1-\\xi_i,\\ \\epsilon_i\u0026gt;=0\\quad i=0,1,2,3,\u0026hellip;,m. $$ 对于这个问题仍可以使用拉格朗日乘子法得出对偶问题：\n软间隔SVM求解 设出目标函数，目标函数涉及一个正则项和松弛变量的惩罚项： $$ L(w, b, \\alpha, \\xi, \\mu) = \\frac{1}{2} |w|^2 + C \\sum_{i=1}^m \\xi_i + \\sum_{i=1}^m \\alpha_i (1 - \\xi_i - y_i (w^T x_i + b)) - \\sum_{i=1}^m \\mu_i \\xi_i $$ 其中，$$\\alpha_i \u0026gt; 0$$，$$\\mu_i \\geq 0$$ 是拉格朗日乘子。\n通过对 $$w$$, $$b$$, $$\\xi$$ 取偏导并令其等于零，可得： $$ w = \\sum_{i=1}^m \\alpha_i y_i x_i $$\n$$ 0 = \\sum_{i=1}^m \\alpha_i y_i $$\n$$ C = \\alpha_i + \\mu_i $$\n将上述结果代入原始目标函数中，可以得到对偶问题： $$ \\max_{\\alpha} \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^T x_j $$\n$$ \\text{s.t.} \\sum_{i=1}^m \\alpha_i y_i = 0, $$\n$$ 0 \u0026lt; \\alpha_i \u0026lt; C, \\quad i = 1, 2, \\ldots, m. $$\n通过求解上述对偶问题，可以得到支持向量机的最优解。\n好的，下面是详细内容，其中所有的数学符号都用 $$ 包围。\n软间隔支持向量机的KKT条件 对于软间隔支持向量机，KKT条件如下：\n$$ \\begin{cases} \\alpha_i \\geq 0, \\ \\mu_i \\geq 0, \\ y_i (f(x_i)) - 1 + \\xi_i \\geq 0, \\ \\alpha_i (y_i (f(x_i)) - 1 + \\xi_i) = 0, \\ \\xi_i \\geq 0, \\ \\mu_i \\xi_i = 0. \\end{cases} $$\n$$\\alpha_i \\geq 0$$：拉格朗日乘子必须是非负的。 $$\\mu_i \\geq 0$$：用于松弛变量的拉格朗日乘子必须是非负的。 $$y_i (f(x_i)) - 1 + \\xi_i \\geq 0$$：这是约束条件，确保样本 $$i$$ 的预测结果满足软间隔约束。 $$\\alpha_i (y_i (f(x_i)) - 1 + \\xi_i) = 0$$：这是互补松弛条件，意味着如果 $$\\alpha_i$$ 非零，那么约束 $$y_i (f(x_i)) - 1 + \\xi_i = 0$$ 必须严格成立。 $$\\xi_i \\geq 0$$：松弛变量必须是非负的。 $$\\mu_i \\xi_i = 0$$：这是互补松弛条件，意味着如果 $$\\xi_i$$ 非零，那么 $$\\mu_i$$ 必须是零，反之亦然。 对于任何训练样本 $$(x_i, y_i)$$，总有 $$\\alpha_i = 0$$ 或 $$y_i (f(x_i)) - 1 + \\xi_i = 0$$。若 $$y_i (f(x_i)) - 1 + \\xi_i \u0026gt; 0$$，则 $$\\alpha_i = 0$$，表明该样本对优化目标无影响；若 $$\\alpha_i \u0026gt; 0$$，则表明 $$y_i (f(x_i)) - 1 + \\xi_i = 0$$，即该样本是支持向量。\n这些KKT条件描述了软间隔支持向量机的优化问题的解的性质，确保模型在满足软间隔约束的同时，最大化分类间隔。\n最终通过求解$\\alpha_i$可以得到$w^T$和$b$的最佳值。\n阅读材料 拉格朗日对偶问题 KKT条件：结合原始约束条件，拉格朗日式的约束（求导），拉格朗日乘子的非负性以及互补松弛性\n投影定理 在有限维的原始空间中，许多数据分布是非线性的，无法通过简单的线性分类器进行有效分类。通过将数据映射到更高维的特征空间，可以利用更高维空间的特性来解决这些问题。以下是一些关于为什么在有限维空间中有一个更高维的特征空间的原因：\nHilbert空间 在许多情况下，原始有限维空间的数据可以被映射到一个无限维的Hilbert空间。在这个空间中，许多复杂的非线性关系可以通过简单的线性关系来描述。这是函数空间和再生核Hilbert空间（RKHS）概念的基础。\n根据投影定理，任何有限维空间都可以嵌入到更高维的空间中。这个更高维的空间包含了原始空间的所有特征，同时还可以容纳更多的特征，从而使得数据分析和处理变得更加灵活和准确。\n数学证明 考虑一个非线性映射函数 ($\\phi$)，它将原始空间 ($\\mathcal{X}$) 中的点映射到高维特征空间 ($\\mathcal{H}$)，即：\n$$ \\phi: \\mathcal{X} \\to \\mathcal{H} $$\n如果 $\\mathcal{H}$ 是一个Hilbert空间，则存在一个核函数 ($k$)，使得：\n$$ k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle_{\\mathcal{H}} $$\n这个核函数 ($k$) 定义了一个更高维的特征空间，而这个特征空间可能是无限维的。\n总之，在有限维空间中，通过合适的映射函数或核函数，可以构造一个更高维的特征空间，以解决原始空间中的非线性问题。这种技术在机器学习中非常重要，特别是在支持向量机和其他核方法中。\n关于核函数的性质 半正定矩阵 一个矩阵 ( $K$) 被称为半正定（positive semi-definite），如果对于任意非零向量 ( $\\mathbf{z}$ )，都有：\n$$ \\mathbf{z}^T K \\mathbf{z} \\geq 0 $$\n也就是说，对于任何向量 ( $\\mathbf{z} $)，当它与矩阵 ( $K$ ) 进行二次型运算时，结果总是非负的。半正定矩阵的一个重要性质是它的特征值非负。\n再生核希尔伯特空间（RKHS） 每一个核函数都隐含地定义了一个再生核希尔伯特空间（$RKHS$）。在这个空间中，核函数具有“再生性”，即对于任意函数 ($f$) 属于这个空间，都有：\n$$ f(x) = \\langle f, \\kappa(x, \\cdot) \\rangle_{\\mathcal{H}} $$\n这意味着在 RKHS 中，通过核函数可以很方便地计算内积，从而进行各种线性操作。\n这段话的意思是，如果我们有一个半正定的核矩阵 (K)，那么我们总能找到一个特征映射函数 ($\\phi$)，将原始数据点映射到一个高维的特征空间，并且在这个特征空间中，核函数的计算就等同于特征映射后的内积。换句话说，任何一个符合条件的核函数都隐含地定义了一个再生核希尔伯特空间（RKHS）。\n核函数与 RKHS 之间的关系 根据 Mercer 定理，任何一个满足条件的半正定核函数都可以视作某个高维空间（可能是无限维空间）中的内积，即存在一个特征映射函数 ($\\phi$)，使得：\n$$ \\kappa(x, y) = \\langle \\phi(x), \\phi(y) \\rangle_{\\mathcal{H}} $$\n(与RKHS对比)这意味着，我们可以通过核函数 ($\\kappa$) 定义一个特征映射 ($\\phi$)，将输入数据从原始空间 ($\\mathcal{X}$) 映射到一个高维的特征空间 ($\\mathcal{H}$)。在这个高维空间中，核函数 ($\\kappa(x, y)$) 就是特征向量 ($\\phi(x)$) 和 ($\\phi(y)$) 之间的内积。\n从半正定矩阵到特征映射 给定一个半正定的核矩阵 ($K$)，根据特征值分解（Eigenvalue Decomposition），我们可以将 ($K$) 分解为：\n$$ K = Q \\Lambda Q^T $$\n其中，(Q) 是特征向量矩阵，($\\Lambda$) 是特征值对角矩阵。我们可以通过特征向量和特征值构造特征映射 ($\\phi$)，使得 ($\\phi(x_i)$) 在高维空间中的表示满足核矩阵的半正定性要求。\n正则化 含有替代损失函数的SVM目标函数 我们可以将 0/1 损失函数替换成别的替代损失函数，从而得到不同的学习模型。这些模型的性质与所用的替代函数直接相关，目标函数可以写为：\n$$ \\min_f \\Omega(f) + C \\sum_{i=1}^m \\ell(f(x_i), y_i) $$\n其中：\n$$\\Omega(f)$$ 称为“结构风险”（structural risk），用于描述模型 $$f$$ 的某些性质。 $$\\sum_{i=1}^m \\ell(f(x_i), y_i)$$ 称为“经验风险”（empirical risk），用于描述模型 $$f$$ 在训练数据上的误差。 $$C$$ 是一个正的常数，用于平衡结构风险和经验风险的权重。 结构风险和经验风险 $$\\Omega(f)$$ 用于描述模型的复杂度或平滑度。常见的 $$\\Omega(f)$$ 形式包括 $$L_p$$ 范数（例如，$$L_2$$ 范数 $$|w|^2$$）等。 $$\\ell(f(x_i), y_i)$$ 是损失函数，用于衡量模型在样本 $$x_i$$ 上的预测值 $$f(x_i)$$ 与真实标签 $$y_i$$ 之间的差异。 正则化 在机器学习中，正则化用于防止过拟合，确保模型在训练数据上的良好表现能够推广到未见过的数据上。目标函数中的正则化项 $$\\Omega(f)$$ 可以帮助控制模型的复杂度。\n综上所述，含有替代损失函数的SVM目标函数通过平衡结构风险和经验风险，优化模型的性能，确保在训练数据和新数据上都有良好的表现。\n支持向量回归(SVR) 支持向量回归模型的解 支持向量不仅能用作分类问题，在回归问题上也能有不错的表现。\n类似线性回归模型，在线性回归模型中我们计算样本到线性超平面的均差来衡量损失从而得到最好的线性超平面。在支持向量回归中，我们使用间隔的定义来计算样本到超平面的“距离”。\n设$\\epsilon$是样本能容忍的$f(x)$与$y$之间的偏差，仅当样本点在$f(x)+\\epsilon$和$f(x)-\\epsilon$之间再计算误差。\n于是SVR问题可以抽象为： $$ \\min_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m\\ell_i(f(x_i)-y_i)\\ \\text{其中C为正则化常数}，\\ell_{\\epsilon}是\\epsilon 的不敏感损失函数\\ \\ell_{\\epsilon}(z)=\\left{ \\begin{matrix} 0,\\quad if|z|\u0026lt;=\\epsilon;\\ |z|-\\epsilon,\\quad otherwise.\\ \\end{matrix} \\right. $$ 引入松弛变量$\\xi_i$和$\\hat\\xi_i$(两侧松弛情况完全可能不同)，得到一般式： $$ \\min_{w,b,\\xi_i,\\hat\\xi_i}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m(\\xi_i+\\hat\\xi_i)\\ s.t. f(x_i)-y_i\u0026lt;=\\epsilon+\\xi_i,\\ y_i-f(x_i)\u0026lt;=\\epsilon+\\hat\\xi_i,\\ \\xi_i\u0026gt;=0,\\hat\\xi_i\u0026gt;=0,\\quad i=1,2,\u0026hellip;,m. $$ 再通过拉格朗日乘子法转化为对偶问题: $$ L(w,b,\\alpha,\\hat\\alpha,\\xi,\\hat\\xi,\\mu,\\hat\\mu)= \\ \\frac{1}{2}||w||^2+C\\sum_{i=1}^m(\\xi_i+\\hat\\xi_i) + \\sum_{i=1}^m \\mu_i\\xi_i+\\sum_{i=1}^m\\hat\\mu_i\\hat\\xi_i\\ +\\sum_{i=1}^m\\alpha_i(\\epsilon+\\xi_i-f(x_i)+y_i)+\\sum_{i=1}^m\\hat\\alpha_i(y_i-f(x_i-\\epsilon-\\hat\\xi_i)\\ . $$ 求偏导得： $$ w=\\sum_{i=1}^m(\\hat\\alpha_i-\\alpha_i)x_i\\ , \\0 = \\sum_{i=1}^m(\\hat\\alpha_i-\\alpha_i)\\ , \\C=\\alpha_i+\\mu_i\\ , \\C=\\hat\\alpha_i+\\hat\\mu_i\\ . $$\n代入原拉格朗日乘子式，对于支持向量回归（SVR），其对偶问题为：\n$$ \\max_{\\alpha, \\hat{\\alpha}} \\sum_{i=1}^m y_i (\\alpha_i - \\hat{\\alpha_i}) - \\epsilon (\\alpha_i + \\hat{\\alpha_i}) - \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m (\\alpha_i - \\hat{\\alpha_i})(\\alpha_j - \\hat{\\alpha_j}) x_i^T x_j $$\n$$ s.t.\\quad \\sum_{i=1}^m (\\alpha_i - \\hat{\\alpha_i}) = 0 $$\n$$ 0 \\leq \\alpha_i \\leq C $$\n$$ 0 \\leq \\hat{\\alpha_i} \\leq C $$\n根据 KKT 条件，上述问题需要满足以下约束：\n$$ \\begin{cases} \\alpha_i (f(x_i) - y_i - \\epsilon - \\xi_i) = 0, \\ \\hat{\\alpha_i} (y_i - f(x_i) - \\epsilon - \\hat{\\xi_i}) = 0, \\ \\alpha_i \\hat{\\alpha_i} = 0, \\ \\xi_i \\hat{\\xi_i} = 0, \\ (C - \\alpha_i) \\xi_i = 0, \\ (C - \\hat{\\alpha_i}) \\hat{\\xi_i} = 0 \\end{cases} $$\n$$\\alpha_i (f(x_i) - y_i - \\epsilon - \\xi_i) = 0$$: 这个条件表示当 $$\\alpha_i$$ 非零时，约束 $$f(x_i) - y_i - \\epsilon - \\xi_i = 0$$ 必须严格成立。 $$\\hat{\\alpha_i} (y_i - f(x_i) - \\epsilon - \\hat{\\xi_i}) = 0$$: 这个条件表示当 $$\\hat{\\alpha_i}$$ 非零时，约束 $$y_i - f(x_i) - \\epsilon - \\hat{\\xi_i} = 0$$ 必须严格成立。 $$\\alpha_i \\hat{\\alpha_i} = 0$$: 这个条件表示 $$\\alpha_i$$ 和 $$\\hat{\\alpha_i}$$ 不能同时为非零，即它们互斥。 $$\\xi_i \\hat{\\xi_i} = 0$$: 这个条件表示 $$\\xi_i$$ 和 $$\\hat{\\xi_i}$$ 不能同时为非零，即它们互斥。 $$(C - \\alpha_i) \\xi_i = 0$$: 这个条件表示当 $$\\xi_i$$ 非零时，$$\\alpha_i$$ 必须为 $$C$$。 $$(C - \\hat{\\alpha_i}) \\hat{\\xi_i} = 0$$: 这个条件表示当 $$\\hat{\\xi_i}$$ 非零时，$$\\hat{\\alpha_i}$$ 必须为 $$C$$。 最后得到$\\text{SVR}$解： $$ f(x)=\\sum_{i=1}^m(\\hat\\alpha_i-\\alpha_i)x_i^Tx+b $$ 若考虑非线性样本空间分步，则可以有映射$\\phi(x_i)=w^Tx_i+b$，$L(·)$对$w$求偏导为 $$ w=\\sum_{i=1}^m(\\hat\\alpha_i-\\alpha_i)\\phi(x_i)\\ . $$ 最终解为： $$ f(x)=\\sum_{i=1}^m(\\hat\\alpha_i-\\alpha_i)\\kappa(x,x_i)+b $$ 其中$\\kappa(x_i,x_j)=\\phi(x_i)^T \\phi(x_j)$\n核方法与核线性扩展 令 $\\mathcal{H}$ 为核函数 $k$ 对应的再生核希尔伯特空间 (RKHS)， $|h|_{\\mathcal{H}}$ 表示 $\\mathcal{H}$ 空间中函数 $h$ 的范数。对于任意单调递增函数 $\\phi: [0, \\infty) \\rightarrow [0, \\infty)$ 和任意非负损失函数 $L: \\mathbb{R}^m \\rightarrow [0, \\infty)$，优化问题\n$$ \\min_{h \\in \\mathcal{H}} \\phi(|h|_{\\mathcal{H}}) + L(h(x_1), h(x_2), \\ldots, h(x_m)) \\tag{6.57} $$\n的解总可写为\n$$ h^*(x) = \\sum_{i=1}^{m} \\alpha_i k(x, x_i). \\tag{6.58} $$\n表示定理对损失函数没有限制，对正则化项仅要求单调递增，甚至不要求是凸函数。这意味着对于一般的损失函数和正则化项，优化问题的最优解 $h^*(x)$ 都可表示为核函数 $k(x, x_i)$ 的线性组合。这显示出核函数的巨大威力。\n人们发展出一系列基于核函数的学习方法，统称为“核方法”（kernel methods）。最常见的，是通过“核化”（即引入核函数）来将线性学习器拓展为非线性学习器。下面我们以线性判别分析为例，来演示如何通过核化对其进行非线性拓展，从而得出“核线性判别分析”（Kernelized Linear Discriminant Analysis，简称 KLDA）。\n我们先假设可通过某种映射函数 $\\phi: X \\rightarrow \\mathcal{F}$ 将样本映射到一个特征空间 $\\mathcal{F}$，然后在 $\\mathcal{F}$ 中执行线性判别分析，以求得\n$$ h(x) = \\omega^T \\phi(x). \\tag{6.59} $$\n类似于式LDA，KLDA 的学习目标是\n$$ \\max_{\\omega} J(\\omega) = \\frac{\\omega^T S_b \\omega}{\\omega^T S_w \\omega}, \\tag{6.60} $$\n其中 $S_b$ 和 $S_w$ 分别为训练样本在特征空间 $\\mathcal{F}$ 中的类间散度矩阵和类内散度矩阵。令 $X_i$ 表示第 $i$ 类样本的集合，其样本数为 $m_i$；总样本数为 $m = m_0 + m_1$，类样本在特征空间 $\\mathcal{F}$ 中的均值为\n$$ \\mu_i = \\frac{1}{m_i} \\sum_{x \\in X_i} \\phi(x), $$\n两个散度矩阵分别为\n$$ S_b = \\sum_{i=0}^{1} m_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T, $$\n$$ S_w = \\sum_{i=0}^{1} \\sum_{x \\in X_i} (\\phi(x) - \\mu_i)(\\phi(x) - \\mu_i)^T. $$\n通常我们难以知道映射 $\\phi$ 的具体形式，因此使用核函数 $k(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$ 来隐式地表达这个映射和特征空间 $\\mathcal{F}$。把 $J(\\omega)$ 作为优化问题 的损失函数，再令 $h(x) = \\sum_{i=1}^{m} \\alpha_i k(x, x_i)$，由表示定理，函数 $h(x)$ 可写为\n$$ h(x) = \\sum_{i=1}^{m} \\alpha_i k(x, x_i), \\tag{6.64} $$\n于是由式$h(x) = \\omega^T \\phi(x).$可得\n$$ \\omega = \\sum_{i=1}^{m} \\alpha_i \\phi(x_i). \\tag{6.65} $$\n令 $K \\in \\mathbb{R}^{m \\times m}$ 为核函数 $k$ 所对应的核矩阵，其中 $K_{ij} = k(x_i, x_j)$。$1_i \\in {0, 1}^{m \\times 1}$ 为第 $i$ 类样本的指示向量，即 $1_i$ 的第 $j$ 个分量为 1 当且仅当第 $j$ 个样本属于第 $i$ 类，否则为 0。再令\n$$ \\hat\\mu_0=\\frac{1}{m_0}K1_{0},$$\n$$ \\hat\\mu_1=\\frac{1}{m_1}K1_{0},$$\n$$ M = (\\mu_0 - \\mu_1)(\\mu_0 - \\mu_1)^T, \\tag{6.67} $$\n$$ N = K^T K - \\sum_{i=0}^{1} \\mu_i \\mu_i^T. \\tag{6.68} $$\n于是，式 $ \\max_{\\omega} J(\\omega) = \\frac{\\omega^T S_b \\omega}{\\omega^T S_w \\omega}$等价为(推导参考阅读资料)\n$$ J(\\alpha) = \\frac{\\alpha^T M \\alpha}{\\alpha^T N \\alpha}. \\tag{6.69} $$\n显然，使用线性判别分析求解方法即可得到 $\\alpha^*$(参考阅读资料的核对数几率回归)\n进而可由式 $h(x) = \\sum_{i=1}^{m} \\alpha_i k(x, x_i), \\tag{6.64}$ 得到投影函数 $h(x)$。\n阅读材料(核方法和KLDA) ","date":"2024-08-27T11:35:03+08:00","permalink":"http://localhost:1313/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80svm/","title":"【深度学习基础】SVM"},{"content":"心与画、梦与狂咲、诗 わたくしといふ現象は 仮定された有機交流電燈の ひとつの青い照明です。\n称为”我“的现象 是被假定的有机交流电灯的 一盏青色照明。\n美是什么？心是何物，物质呢？艺术是什么？自然呢？自然模仿艺术？艺术模仿自然？孤立片面静止的神？因果交流的樱之艺术家？\n乘着刚通关的余韵，将一些愚人之笔落于此，我对美不曾深入思考过，但对神的存在，我曾考虑许多。\n故事开头以某位重要人物在春天的死去起笔，注定这部故事带有的独特氛围。春，死，生命，一种缺憾而忧郁的氛围，平淡的反应，对春的忧郁。被樱花包围的、棺材中的他，他是父亲，“他所看到的世界的消失点”，对于“我”，对于玩家，恰好是故事的开始。这样，我们也被樱花包围着，和夏目蓝拥抱的温暖的触感，带着春的忧郁，步入新生的故事。\n我不曾深入了解过艺术以及艺术史，人物大部分都不是耳熟能详的，宫泽贤治，莫奈，马奈，中原中也\u0026hellip;我注定现在无法理解其中一些事物，不过我相信现在的文学作品大多是解构主义，原作者的意图透过SCA自的心象，我应该领略这点，不过我依然很理解，至少写下这篇随记的时候是这样。\n即使这样，我也能感受到其中的音色，那是作品传达到的表现，读不懂，无法理解，没有耐心，我也能感受到其中的音色，真正的天才忘却才能，透过画看到看不到的，透过诗体会到音色，这既是故事的表达之一，也是故事的展开手段之一\n以死亡开始的故事，注定会在死者的阴影里行进。作为那个艺术家的后裔，草薙直哉承担了两代人的主要矛盾，如果有意思思考对话，能撇见故事的主角的被封锁的过去。这是故事的展开点。在故事里，每个人都不是形式上的简单铺开（即，卖人设），与更多文学作品类似，每个人都是一种思考，或对立或辅佐，每次对话都是思考的交锋，他们的对话很复杂，有的戏言一般轻蔑，有的辩论一般猛烈，里面的人物说话带有作者的思考，因而显得哲学，即便是不起眼的角色，都被赋予了这样的权能，如同吟诗一般。随着故事行进，记忆铺陈开来，之前“画中看到看不到”的过去，也随着因果交流慢慢叙来。说实话，每一篇所引用以及解构的思想都有差别，整体却和谐，导致每当出现思考对话时，总感觉是带着缺憾和忧郁，最后飞舞在故事开头或结尾的樱花里。\n果然这时候还是应该写情绪最深的几个地方，一大概是里奈篇的多视角进行，这个故事单独拿出来我也很喜欢，我喜欢这种直来直往的情绪交流，白色毒蘑菇与被魅惑的狼，ZYPREESEN与樱花，死亡与生命，这是草薙直哉放弃右手求得的奇迹，比真正的千年樱更加真实的奇迹，我在思考整个故事最后抛出的疑问-“misukura rin寄宿的神（即，孤立静止片面封闭）”与“Naoya的弱小的神（因果交流）”时，产生了积极影响。比起“绝对的神”来说，草薙直哉，或者每个人的传达的心象素描，那或许才是真正的神。话题回到这一章，这一章的话题极具敏感性和矛盾，天生的同性恋，死亡气息的白色毒蘑菇，诱惑生命力满满的狼，小红帽被狼吃掉是因为毫无防备，如果小红帽身着白色，是否会有一样的结局，白色是自然界危险的颜色，然而毒性巨大的白蘑菇却称为“白色天使”，仿佛诱惑一般，诱惑了优美这匹生命力满满的狼，本来，结局应该是狼将小红帽吃掉。然而，在后来的格林童话中，猎人杀掉了狼，拯救了小红帽。猎人便是樱之艺术家，草薙直哉。或许是也是被少女的死亡气息吸引，这是一种绝对的美感，但是对于他来说，如同将“神”的画覆盖一般，他拥有“因果交流”的心象，无论何时在心中狂放的千年樱，让他完成一个又一个奇迹（也让他失去了很多），他将樱花盛开与死之少女的ZYPRESSEN之上，ZYPRESSEN是死亡的象征，出自宫泽贤治的《春与修罗》，失去一切的修罗，类似的，在他的另一本书《银河铁道之夜》，有一段关于“天蝎之火”的故事，天蝎座心宿二，位于天蝎的心脏，全天最孤独的一等星，天蝎燃烧心脏发出赤色火焰，孤独又猛烈，如同梵高的《有丝柏的道路一般》，黑色火焰，隔断了星与月。有意思的是ZYPRESSEN即是丝柏。于是，少女跨越了死亡认识到了生命，并且深深爱上了这个樱之艺术家。于是，这个三角关系的悲剧注定发生，诱惑狼的死亡少女担起与狼共存的命运，同时，深深爱着这位樱之艺术家。于是也像故事最后那样，当相爱的两人爱意显现，优美化作天蝎，化作ZYPRESSEN，一团漆黑的火焰，深深的思念引发了真正的奇迹，即便思念之深，她也明白应该让两人相爱，她将仅有的思念化作的千年樱，作为祝福献给了死之少女与樱之艺术家。优美打心里喜欢直哉，也痛恨他，痛恨ZYPRESSEN，却化作ZYPRESSEN。她爱和痛恨着这样的修罗，这样的幸福王子。这是故事第一次完全展现“奇迹”这个要素，也牵扯出了诸多故事的核心要素，比起写优美，更像对称写直哉。以这张为分割，更多故事的线索得以浮现。\n要说震撼的地方，果然还是要提misakura rin和Naoya关于“艺术，自然，神”的对话，我无法做出太多的理解，不过我对rin抱有强烈的不满感，这或许源自我对孤立的神的本性不理解，艺术模仿自然，王尔德定义了唯美，与其说是定义，更不如说发现，在这之前，自然模仿艺术，亚里士多德提出了这样自然的言论，哪个是正论，这恐怕不是我能理解到的，也许这两者都正确或者不正确，rin的神是绝对的神的概念，是美这一所谓人造概念或者说自然概念的具现化，我无法通过推理给出更多可解释性，也许美本来就是这种东西，于是我在此无法给出论断，又或者说猜想，也许以后我会有能力来解释，并且对自己的思考自圆其说。\n我果然不是勤劳的人，暂时搁笔。我可能单纯享受这样体现自我价值的思考，我想留下一些想法，也许我以后会赞同，也许不赞同，然后给以后的自己一些提示，我觉得值得写的东西，仅仅是现在给了我强烈地情绪，所以我写了，你要我评论他（作品，人物）的好坏，我没资格也没必要，也许某一天我也会意识到批评的重要性，不过是后话，随笔，情绪而为，No matter！Never Mind!\nO wende, wende deinen Lauf/Im Thale blüht der Frühling auf! Frühlingsbeginn Abend picapica 兔子不过是迷上了月亮的疯狂；迷上即失败\nOlympia 愛するものが死んだ時には、 自殺しなけあなりません。\n愛するものが死んだ時には、 それより他に、方法がない\n挚爱的人死了，我必须随着去了\n挚爱的人死了，除此之外别无他法\nZYPREESEN ZYPRESSEN 春のいちれつ ZYPRESSEN しづかにゆすれ\tZYPRESSEN いよいよ黒\nZYPREESEN 春天的队列\tZYPREESEN 静静摇曳 ZYPREESEN\t愈发黑沉\na nice derangement of epitaph 墓志铭的美妙混乱\nWhat is mind? What is matter? What is mind ？No matter.\nWhat is matter ? Never mind.\nThe happy prince and other tales 冬天快到了，燕子啊，快快飞向南方\n于樱之森下漫步 因果交流的、樱之艺术家\n于樱之森上起舞 它若是虚无的话，那虚无本身不过如此\n神\n心象\n画\n樱之艺术家\n参考 春与修罗\n春日狂想\n喜鹊\n奥林比亚\n我是谁，我来自哪里，要去哪里\nZYPRESSEN\n小红帽\n心象素描\n美\n神\n唯美\n纯粹理性批判\n艺术模仿自然\n自然模仿艺术\n数字与美\n幸福王子\n王尔德\n高更\n马奈\n宫泽贤治\n莫奈\n","date":"2024-08-24T21:47:19+08:00","image":"http://localhost:1313/img/4featureimg/sakuranoshi.jpg","permalink":"http://localhost:1313/p/%E5%BF%83%E4%B8%8E%E7%94%BB%E6%A2%A6%E4%B8%8E%E7%8B%82%E5%92%B2%E8%AF%97/","title":"心与画、梦与狂咲、诗"}]